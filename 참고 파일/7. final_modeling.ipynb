{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Sw1aQcBW7B",
        "outputId": "24efb382-df04-4c2a-af52-9b7000d2722b"
      },
      "outputs": [],
      "source": [
        "def get_environment():\n",
        "    try:\n",
        "        # Check if the environment is Google Colab\n",
        "        import google.colab\n",
        "        return \"colab\"\n",
        "    except ImportError:\n",
        "        # Environment is not Goㅇogle Colab, assume it is local\n",
        "        return \"local\"\n",
        "\n",
        "environment = get_environment()\n",
        "\n",
        "if environment == \"colab\":\n",
        "    colab_path = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !pip install finance-datareader\n",
        "elif environment == \"local\":\n",
        "    colab_path = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy305SdkBBfh"
      },
      "source": [
        "함수 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P8bI8YMGBBfk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "import FinanceDataReader as fdr\n",
        "from datetime import timedelta\n",
        "\n",
        "# Neural Network library\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras.models import load_model\n",
        "from keras import callbacks\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.layers import Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"bmh\")\n",
        "\n",
        "# 한글 폰트 사용을 위해서 세팅\n",
        "from matplotlib import font_manager, rc\n",
        "plt.rc('font', family='NanumGothic') # For Windows\n",
        "%matplotlib inline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MV 모델 벤치마크"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_portfolio_std(weights, mean_returns, cov):\n",
        "    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov, weights)))\n",
        "    return portfolio_std\n",
        "\n",
        "def calc_neg_sharpe(weights, mean_returns, cov, rf):\n",
        "    portfolio_return = np.sum(mean_returns * weights)\n",
        "    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov, weights)))\n",
        "    sharpe_ratio = (portfolio_return - rf) / portfolio_std\n",
        "    return -sharpe_ratio\n",
        "\n",
        "def calc_portfolio_VaR(weights, mean_returns, cov, alpha):\n",
        "    portfolio_return = np.sum(mean_returns * weights)\n",
        "    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov, weights)))\n",
        "    portfolio_var = abs(portfolio_return - (portfolio_std * stats.norm.ppf(1 - alpha)))\n",
        "    return portfolio_return, portfolio_std, portfolio_var\n",
        "\n",
        "def portfolio_optimization_std(w, r, cov):\n",
        "    constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]\n",
        "    args = (r, cov)\n",
        "    bound = (0.0, 1.0)\n",
        "    bounds = tuple(bound for asset in range(len(w)))\n",
        "    optimization_result = minimize(calc_portfolio_std, w, args=args, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "    \n",
        "    return optimization_result\n",
        "\n",
        "def portfolio_optimization_sharpe(w, r, cov, rf):\n",
        "    constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]\n",
        "    args = (r, cov, rf)\n",
        "    bound = (0.0, 1.0)\n",
        "    bounds = tuple(bound for asset in range(len(w)))\n",
        "    optimization_result = minimize(calc_neg_sharpe, w, args=args, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "    \n",
        "    return optimization_result\n",
        "\n",
        "def portfolio_optimization_Var(w, r, cov, alpha):\n",
        "    constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]\n",
        "    args = (r, cov, alpha)\n",
        "    bound = (0.0, 1.0)\n",
        "    bounds = tuple(bound for asset in range(len(w)))\n",
        "    optimization_result = minimize(calc_portfolio_VaR, w, args=args, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "    \n",
        "    return optimization_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg3XcN35BBfl"
      },
      "source": [
        "## 2002-01-01 부터 학습하여 2018년 이후의 백테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KOPXfAEYBBfl"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(colab_path + \"top50-Stock_LogReturn.csv\", index_col=\"Date\")\n",
        "train, test = train_test_split(df, test_size=0.2, shuffle=False)\n",
        "\n",
        "train_len = len(train)\n",
        "test_len = len(test)\n",
        "validation_set_len = 80\n",
        "validation_set_split_point = 60\n",
        "\n",
        "xc_train = np.empty((train_len - validation_set_len, 60, 50))\n",
        "xf_train = np.empty((train_len - validation_set_len, 20, 50))\n",
        "xc_test = np.empty((test_len - validation_set_len, 60, 50))\n",
        "xf_test = np.empty((test_len - validation_set_len, 20, 50))\n",
        "\n",
        "for idx in range(train_len - validation_set_len):\n",
        "    temp_xc_train = train[idx : idx + validation_set_split_point]\n",
        "    temp_xf_train = train[idx + validation_set_split_point : idx + validation_set_len]\n",
        "\n",
        "    xc_train[idx] = temp_xc_train\n",
        "    xf_train[idx] = temp_xf_train\n",
        "\n",
        "for idx in range(test_len - validation_set_len):\n",
        "    temp_xc_test = test[idx : idx + validation_set_split_point]\n",
        "    temp_xf_test = test[idx + validation_set_split_point : idx + validation_set_len]\n",
        "\n",
        "    xc_test[idx] = temp_xc_test\n",
        "    xf_test[idx] = temp_xf_test\n",
        "    \n",
        "# 월간 수익률 정도의 스케일로 변환한다\n",
        "xc_train = xc_train.astype('float32') * 20\n",
        "xf_train = xf_train.astype('float32') * 20\n",
        "xc_test = xc_test.astype('float32') * 20\n",
        "xf_test = xf_test.astype('float32') * 20\n",
        "\n",
        "N_TIME = xc_train.shape[1]\n",
        "N_FUTURE = xf_train.shape[1]\n",
        "N_STOCKS = xf_train.shape[2]\n",
        "\n",
        "# 학습 데이터는 shuffling 한다.\n",
        "xc_train, xf_train = shuffle(xc_train, xf_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hb9wVW4WBBfm"
      },
      "outputs": [],
      "source": [
        "# over confidence를 제어할 조절 변수 정의\n",
        "GAMMA_CONST = 0.1\n",
        "REG_CONST = 0.0\n",
        "\n",
        "# 최적 포트폴리오를 구축할 목표 함수를 정의한다.\n",
        "# MPN에서는 이 함수를 loss로 이용한다. max(objective) = min(-objective)\n",
        "# y_true = model.fit()에서 전달된 N_FUTURE일 후의 수익률 (xf_train)이 들어온다.\n",
        "# y_pred = 마코비츠 네트워크의 출력이 전달된다. (keras 내부 기능)\n",
        "\n",
        "def markowitz_objective(y_true, y_pred):\n",
        "    W = y_pred      # 마코비츠 네트워크의 출력\n",
        "    xf_rtn = y_true\n",
        "    W = tf.expand_dims(W, axis = 1)\n",
        "    R = tf.expand_dims(tf.reduce_mean(xf_rtn, axis = 1), axis = 2)\n",
        "    C = tfp.stats.covariance(xf_rtn, sample_axis=1)\n",
        "\n",
        "    rtn = tf.matmul(W, R)  \n",
        "    vol = tf.matmul(W, tf.matmul(C, tf.transpose(W, perm = [0, 2, 1]))) * GAMMA_CONST\n",
        "    reg = tf.reduce_sum(tf.square(W), axis = -1) * REG_CONST\n",
        "    objective = rtn - vol - reg\n",
        "    \n",
        "    return -tf.reduce_mean(objective, axis=0)\n",
        "\n",
        "# LSTM으로 Markowitz 모델을 생성한다.\n",
        "xc_input = tf.keras.Input(batch_shape = (None, N_TIME, N_STOCKS))\n",
        "h_lstm = LSTM(64, dropout = 0.5, kernel_regularizer=l2(0.005))(xc_input)\n",
        "y_output = Dense(N_STOCKS, activation='relu')(h_lstm)  # linear projection\n",
        "\n",
        "# 마코비츠의 최적 weights\n",
        "y_output = Activation('softmax')(y_output)\n",
        "model = tf.keras.Model(xc_input, y_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E45NTxs_BBfm"
      },
      "source": [
        "prophet이 계속 학습하면 계속 Loss가 줄어듦으로 에포크를 500으로 통일 제한한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk4-VnycBBfm",
        "outputId": "97500921-2911-4ad3-d848-8fd7d90c0026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "130/130 [==============================] - 9s 12ms/step - loss: 0.4058 - val_loss: 0.4039\n",
            "Epoch 2/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3944 - val_loss: 0.3926\n",
            "Epoch 3/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.3833 - val_loss: 0.3817\n",
            "Epoch 4/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3725 - val_loss: 0.3710\n",
            "Epoch 5/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3620 - val_loss: 0.3606\n",
            "Epoch 6/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3517 - val_loss: 0.3505\n",
            "Epoch 7/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.3417 - val_loss: 0.3406\n",
            "Epoch 8/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3319 - val_loss: 0.3309\n",
            "Epoch 9/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3224 - val_loss: 0.3215\n",
            "Epoch 10/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3130 - val_loss: 0.3123\n",
            "Epoch 11/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3040 - val_loss: 0.3033\n",
            "Epoch 12/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.2950 - val_loss: 0.2945\n",
            "Epoch 13/500\n",
            "130/130 [==============================] - 1s 11ms/step - loss: 0.2864 - val_loss: 0.2859\n",
            "Epoch 14/500\n",
            "130/130 [==============================] - 1s 11ms/step - loss: 0.2779 - val_loss: 0.2776\n",
            "Epoch 15/500\n",
            "130/130 [==============================] - 1s 9ms/step - loss: 0.2696 - val_loss: 0.2694\n",
            "Epoch 16/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2615 - val_loss: 0.2614\n",
            "Epoch 17/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2536 - val_loss: 0.2536\n",
            "Epoch 18/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2458 - val_loss: 0.2459\n",
            "Epoch 19/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2383 - val_loss: 0.2385\n",
            "Epoch 20/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2309 - val_loss: 0.2312\n",
            "Epoch 21/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.2236 - val_loss: 0.2240\n",
            "Epoch 22/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2166 - val_loss: 0.2171\n",
            "Epoch 23/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2097 - val_loss: 0.2102\n",
            "Epoch 24/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2029 - val_loss: 0.2036\n",
            "Epoch 25/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1964 - val_loss: 0.1971\n",
            "Epoch 26/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1899 - val_loss: 0.1907\n",
            "Epoch 27/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1836 - val_loss: 0.1845\n",
            "Epoch 28/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1775 - val_loss: 0.1784\n",
            "Epoch 29/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1715 - val_loss: 0.1725\n",
            "Epoch 30/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1656 - val_loss: 0.1667\n",
            "Epoch 31/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1598 - val_loss: 0.1611\n",
            "Epoch 32/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1542 - val_loss: 0.1556\n",
            "Epoch 33/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1488 - val_loss: 0.1502\n",
            "Epoch 34/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1434 - val_loss: 0.1449\n",
            "Epoch 35/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1382 - val_loss: 0.1398\n",
            "Epoch 36/500\n",
            "130/130 [==============================] - 1s 9ms/step - loss: 0.1330 - val_loss: 0.1349\n",
            "Epoch 37/500\n",
            "130/130 [==============================] - 1s 10ms/step - loss: 0.1280 - val_loss: 0.1301\n",
            "Epoch 38/500\n",
            "130/130 [==============================] - 1s 9ms/step - loss: 0.1229 - val_loss: 0.1256\n",
            "Epoch 39/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1177 - val_loss: 0.1213\n",
            "Epoch 40/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1125 - val_loss: 0.1168\n",
            "Epoch 41/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1076 - val_loss: 0.1125\n",
            "Epoch 42/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1029 - val_loss: 0.1081\n",
            "Epoch 43/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0984 - val_loss: 0.1039\n",
            "Epoch 44/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.0940 - val_loss: 0.0999\n",
            "Epoch 45/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0898 - val_loss: 0.0959\n",
            "Epoch 46/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0858 - val_loss: 0.0920\n",
            "Epoch 47/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0819 - val_loss: 0.0883\n",
            "Epoch 48/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.0782 - val_loss: 0.0846\n",
            "Epoch 49/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0745 - val_loss: 0.0811\n",
            "Epoch 50/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0710 - val_loss: 0.0776\n",
            "Epoch 51/500\n",
            "130/130 [==============================] - 1s 9ms/step - loss: 0.0675 - val_loss: 0.0743\n",
            "Epoch 52/500\n",
            "130/130 [==============================] - 2s 13ms/step - loss: 0.0642 - val_loss: 0.0710\n",
            "Epoch 53/500\n",
            "130/130 [==============================] - 1s 10ms/step - loss: 0.0610 - val_loss: 0.0678\n",
            "Epoch 54/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0578 - val_loss: 0.0647\n",
            "Epoch 55/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0548 - val_loss: 0.0618\n",
            "Epoch 56/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0589\n",
            "Epoch 57/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0489 - val_loss: 0.0560\n",
            "Epoch 58/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0462 - val_loss: 0.0533\n",
            "Epoch 59/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0435 - val_loss: 0.0507\n",
            "Epoch 60/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.0481\n",
            "Epoch 61/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.0456\n",
            "Epoch 62/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0359 - val_loss: 0.0433\n",
            "Epoch 63/500\n",
            "130/130 [==============================] - 2s 12ms/step - loss: 0.0335 - val_loss: 0.0409\n",
            "Epoch 64/500\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.0312 - val_loss: 0.0387\n",
            "Epoch 65/500\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.0290 - val_loss: 0.0365\n",
            "Epoch 66/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0344\n",
            "Epoch 67/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0248 - val_loss: 0.0324\n",
            "Epoch 68/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0229 - val_loss: 0.0305\n",
            "Epoch 69/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0209 - val_loss: 0.0286\n",
            "Epoch 70/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0191 - val_loss: 0.0268\n",
            "Epoch 71/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0173 - val_loss: 0.0250\n",
            "Epoch 72/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.0156 - val_loss: 0.0234\n",
            "Epoch 73/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0140 - val_loss: 0.0217\n",
            "Epoch 74/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0124 - val_loss: 0.0202\n",
            "Epoch 75/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0109 - val_loss: 0.0187\n",
            "Epoch 76/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0094 - val_loss: 0.0173\n",
            "Epoch 77/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0080 - val_loss: 0.0159\n",
            "Epoch 78/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0067 - val_loss: 0.0146\n",
            "Epoch 79/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.0054 - val_loss: 0.0134\n",
            "Epoch 80/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0121\n",
            "Epoch 81/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0110\n",
            "Epoch 82/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0099\n",
            "Epoch 83/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 8.6401e-04 - val_loss: 0.0089\n",
            "Epoch 84/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -1.5262e-04 - val_loss: 0.0079\n",
            "Epoch 85/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0011 - val_loss: 0.0069\n",
            "Epoch 86/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0020 - val_loss: 0.0060\n",
            "Epoch 87/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0029 - val_loss: 0.0052\n",
            "Epoch 88/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0037 - val_loss: 0.0044\n",
            "Epoch 89/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0045 - val_loss: 0.0036\n",
            "Epoch 90/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0053 - val_loss: 0.0028\n",
            "Epoch 91/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0060 - val_loss: 0.0022\n",
            "Epoch 92/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0066 - val_loss: 0.0015\n",
            "Epoch 93/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0072 - val_loss: 9.2443e-04\n",
            "Epoch 94/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0078 - val_loss: 3.2486e-04\n",
            "Epoch 95/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0084 - val_loss: -2.2474e-04\n",
            "Epoch 96/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0089 - val_loss: -7.5191e-04\n",
            "Epoch 97/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0094 - val_loss: -0.0013\n",
            "Epoch 98/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0098 - val_loss: -0.0017\n",
            "Epoch 99/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0103 - val_loss: -0.0022\n",
            "Epoch 100/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0107 - val_loss: -0.0025\n",
            "Epoch 101/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0110 - val_loss: -0.0028\n",
            "Epoch 102/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0114 - val_loss: -0.0032\n",
            "Epoch 103/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0117 - val_loss: -0.0035\n",
            "Epoch 104/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0120 - val_loss: -0.0039\n",
            "Epoch 105/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0123 - val_loss: -0.0041\n",
            "Epoch 106/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0125 - val_loss: -0.0043\n",
            "Epoch 107/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0127 - val_loss: -0.0046\n",
            "Epoch 108/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0130 - val_loss: -0.0048\n",
            "Epoch 109/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0132 - val_loss: -0.0049\n",
            "Epoch 110/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0134 - val_loss: -0.0050\n",
            "Epoch 111/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0136 - val_loss: -0.0053\n",
            "Epoch 112/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0137 - val_loss: -0.0054\n",
            "Epoch 113/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0139 - val_loss: -0.0055\n",
            "Epoch 114/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0141 - val_loss: -0.0055\n",
            "Epoch 115/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0143 - val_loss: -0.0057\n",
            "Epoch 116/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0144 - val_loss: -0.0058\n",
            "Epoch 117/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0146 - val_loss: -0.0058\n",
            "Epoch 118/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0146 - val_loss: -0.0058\n",
            "Epoch 119/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0148 - val_loss: -0.0060\n",
            "Epoch 120/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0148 - val_loss: -0.0061\n",
            "Epoch 121/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0150 - val_loss: -0.0061\n",
            "Epoch 122/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0151 - val_loss: -0.0061\n",
            "Epoch 123/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0152 - val_loss: -0.0060\n",
            "Epoch 124/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0153 - val_loss: -0.0060\n",
            "Epoch 125/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0154 - val_loss: -0.0062\n",
            "Epoch 126/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0155 - val_loss: -0.0061\n",
            "Epoch 127/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0156 - val_loss: -0.0060\n",
            "Epoch 128/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0156 - val_loss: -0.0063\n",
            "Epoch 129/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0158 - val_loss: -0.0060\n",
            "Epoch 130/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0159 - val_loss: -0.0060\n",
            "Epoch 131/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0158 - val_loss: -0.0060\n",
            "Epoch 132/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0161 - val_loss: -0.0059\n",
            "Epoch 133/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0160 - val_loss: -0.0059\n",
            "Epoch 134/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0160 - val_loss: -0.0059\n",
            "Epoch 135/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0162 - val_loss: -0.0059\n",
            "Epoch 136/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0163 - val_loss: -0.0063\n",
            "Epoch 137/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0162 - val_loss: -0.0060\n",
            "Epoch 138/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0164 - val_loss: -0.0058\n",
            "Epoch 139/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0164 - val_loss: -0.0058\n",
            "Epoch 140/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0166 - val_loss: -0.0057\n",
            "Epoch 141/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0166 - val_loss: -0.0056\n",
            "Epoch 142/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0168 - val_loss: -0.0056\n",
            "Epoch 143/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0170 - val_loss: -0.0056\n",
            "Epoch 144/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: -0.0170 - val_loss: -0.0055\n",
            "Epoch 145/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0173 - val_loss: -0.0054\n",
            "Epoch 146/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: -0.0173 - val_loss: -0.0055\n"
          ]
        }
      ],
      "source": [
        "# MPN을 학습하고 결과를 저장한다.\n",
        "SAVE_MODEL = 'Markowitz_network_final'\n",
        "ealry_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "model.compile(loss = markowitz_objective, optimizer = Adam(learning_rate = 1e-5))\n",
        "hist = model.fit(xc_train, xf_train, epochs=500, batch_size = 32, validation_data = (xc_test, xf_test), callbacks=[ealry_stopping])\n",
        "model.save(SAVE_MODEL + '.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "7dAC0DoOBBfn",
        "outputId": "c5780459-741a-4023-e03d-aa72af05c09e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family ['NanumGothic'] not found. Falling back to DejaVu Sans.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 363ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAAD1CAYAAACFplZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df2zc933f8debdzyR4i/zx41UpIRxFBequwYJ6toIUiTZmERKVtiB4Kw2lsABFGwDZCxDEGzOMCeZvSBpOmwrimw10LhJt7Wu66SZkCpTvcbBWkRJbSs/7dSL5Jk2HR15/KHjr6OOPL73B79maIYSj3y/v/zwc9/XAxCs+0Hy/X1S9If343MnqgoiIiLaH1pCD0BEREQ/x4WZiIhoH+HCTEREtI9wYSYiItpHuDATERHtI1yYiYiI9pF86AEA4Fvf+pYeOHAg9BhERER7YnFxcXJkZKS41WX7YmE+cOAAjh07ltrnHx0dxfDwcGqfPwvY0I4NfbCjHRv6sHS8cOHC6LUuy8Rd2a2traFHiB4b2rGhD3a0Y0MfaXXMxMLc09MTeoTosaEdG/pgRzs29JFWx0wszJOTk6FHiB4b2rGhD3a0Y0MfaXXMxMLM3w7t2NCODX2wox0b+uAtZoNarRZ6hOixoR0b+mBHOzb0kVbHTCzM1Wo19AjRY0M7NvTBjnZs6COtjplYmIeGhkKPED02tGNDH+xox4Y+0uqYiYW5VCqFHiF6bGjHhj7Y0Y4NfaTVcV+8wIi3+89detXpI/kFjP3d2nkPHj8aYqToFQqF0CNEjw19sKMdG/pIq2MmbjHPrvIfoVVXV1foEaLHhj7Y0Y4NfaTVMRMLczG3FHqE6E1NTYUeIXps6IMd7djQR1odM7EwT9X5BhlWvb29oUeIHhv6YEc7NvSRVsdMLMwHW1ZCjxA9bq+wY0Mf7GjHhj64XcqgXeqhR4je0hIfDrBiQx/saMeGPtLqmImFeWzlYOgRosd9j3Zs6IMd7djQB/cxGxzJL4YeIXrc92jHhj7Y0Y4NfaTVMRMLc1VzoUeIXltbW+gRoseGPtjRjg19pNUxEwvz4mpTvo7Knmpvbw89QvTY0Ac72rGhj7Q6ZmJh7s9dDT1C9GZmZkKPED029MGOdmzoI62OmViYy3XebWPV398feoTosaEPdrRjQx9pdczEwtzdwvcetZqbmws9QvTY0Ac72rGhj7Q6ZmJhPiCroUeIHt9Y3Y4NfbCjHRv6SKtjQwuziJwQkedE5KKI3LfF5R8TkWdF5Ici8lciMrzhsrqIfD/5c8Zz+EZxH7Md9z3asaEPdrRjQx/B9jGLSA7AFwC8F8DNAO4WkZs3Xe17AG5R1TcBeAzA5zdcVlXVNyd/bneae0e4j9mO+x7t2NAHO9qxoY+Q+5hvBXBRVZ9X1RqARwDcsfEKqvqEqr6y+n0HwBHfMW24XcqO2yvs2NAHO9qxoY+Q26UOA3hpw+mx5LxrOQXgGxtOt4nIUyLyHRF5/y5mNFvSTDyUniq+sbodG/pgRzs29JFWR9ebkiLyQQC3AHjHhrOHVfVlEXkDgG+KyI9U9dLGj5uYmMCpU6eQz+dRr9dx8uRJnD59GqVSCR0dHcjlcpidnUWxWMT09DRUFcViEePj4+js7AQAzM/PY3BwEOVyGcP5eZTrbTiUr+LKagGHc4vo0xrGVg5idHQUra2t6OnpweTkJHp6elCr1VCtVjE0NIRSqYRCoYCuri5MTU2ht7cX1WoVS0tL65e3tbWhvb0dMzMz6O/vx9zcHGq12vrl7e3tKBQKqFQqGBgYQKVSwfLy8vrluzkmEUFfXx/K5TK6u7tRr9exsLCw/jnTPqaFhQUAaKpj2uvv04svvojFxcWmOqYQ36exsTFUKpWmOqa9/j4tLS013TGF+D4tLi5CVXd1TNddS1V1u8X2rQA+rarHk9OfAABV/eym670LwO8BeIeqTlzjc30JwNdV9bGN558/f16PHTt23Tl24v5zr1r30SnLmNdWAMCDx4+6fZ0sWVhYQEdHR+gxosaGPtjRjg19WDpeuHDh6ZGRkVu2uqyR+3ifBHCTiNwoIgUAdwF41bOrReQtAB4CcPvGRVlEekXkQPL3AQBvA/Dsro7CoC/HrQFWlUol9AjRY0Mf7GjHhj7S6rjtXdmquiIi9wI4ByAH4GFVfUZEHgDwlKqeAfA7ADoB/JmIAMCLyTOwfxnAQyKyirVfAj6nqnu+MLdyH7PZ8vJy6BGix4Y+2NGODX2k1bGhx5hV9SyAs5vO++SGv7/rGh/3bQC/ahnQA/cx23Hfox0b+mBHOzb0wfdjNuA+Zjvue7RjQx/saMeGPvh+zAbz3MdsxieK2LGhD3a0Y0MfaXXMxMK8ko3DTFUulws9QvTY0Ac72rGhj7Q6ZmLFuoHvLmU2OzsbeoTosaEPdrRjQx9pdczEwnx5hS8/Z1UsFkOPED029MGOdmzoI62OmViYi7ml0CNEb3p6OvQI0WNDH+xox4Y+0uqYiYW5RUJPEL/tXiGOtseGPtjRjg19pNUxEwvz5ZW20CNEj3d92bGhD3a0Y0MfvCvb4HC+GnqE6I2Pj4ceIXps6IMd7djQR1odM7Ewz662hh4heq+8SwrtHhv6YEc7NvSRVsdMLMxERESxyMTC3N3CF2y3mp+fDz1C9NjQBzvasaGPtDpmYmF+mfuYzQYHB0OPED029MGOdmzoI62OmViYD+W5j9mqXC6HHiF6bOiDHe3Y0EdaHTOxMK9yy55Z8j7bZMCGPtjRjg19pNUxEwtzuc59zFZ9fX2hR4geG/pgRzs29JFWx0wszIe4j9mMd33ZsaEPdrRjQx+8K9vgymoh9AjR6+7uDj1C9NjQBzvasaGPtDpmYmHOYzX0CNGr1+uhR4geG/pgRzs29JFWx0wszJ0tK6FHiN7CwkLoEaLHhj7Y0Y4NfaTVMRML89jKwdAjRG9oaCj0CNFjQx/saMeGPtLqmImF+Uh+MfQI0SuVSqFHiB4b+mBHOzb0kVbHTCzMy5qJw0xVayvfCMSKDX2wox0b+kirYyZWrOk6n5Vt1dPTE3qE6LGhD3a0Y0MfaXXMxMI8yJfkNJucnAw9QvTY0Ac72rGhj7Q6ZmJh5i1mO/6GbceGPtjRjg198BazQZtwH7NVrVYLPUL02NAHO9qxoY+0OmZiYT7Ifcxm1Spf1tSKDX2wox0b+kirYyYWZu5jtuO+Rzs29MGOdmzoI+g+ZhE5ISLPichFEblvi8s/JiLPisgPReSvRGR4w2X3iMhPkz/3eA7fKO5jtuO+Rzs29MGOdmzoI9g+ZhHJAfgCgPcCuBnA3SJy86arfQ/ALar6JgCPAfh88rF9AD4F4DYAtwL4lIj0+o3fmKvcx2xWKPAJdFZs6IMd7djQR1odG1mxbgVwUVWfV9UagEcA3LHxCqr6hKq+crP0OwCOJH8/DuBxVZ1W1RkAjwM44TN642b57lJmXV1doUeIHhv6YEc7NvSRVsdGFubDAF7acHosOe9aTgH4xi4/NhXFHPcxW01NTYUeIXps6IMd7djQR1od856fTEQ+COAWAO/YycdNTEzg1KlTyOfzqNfrOHnyJE6fPo1SqYSOjg7kcjnMzs6iWCxienoaqopisYjx8XF0dnYCAObn5zE4OIhyuYzh/DzK9TYcyldxZbWAVQWOts5hbOUgRkdH0draip6eHkxOTqKnpwe1Wg3VahVDQ0MolUooFAro6urC1NQUent7Ua1WsbS0tH55W1sb2tvbMTMzg/7+fszNzaFWq61f3t7ejkKhgEqlgoGBAVQqFSwvL69fvptjEhH09fWhXC6ju7sb9XodCwsL658z7WMCgCtXrjTVMe319+nq1av42c9+1lTHFOL7VK/XMTo62lTHtNffp9bWVkxMTDTVMYX4PrW0tGBmZmZXx3TdtVRVt1ts3wrg06p6PDn9CQBQ1c9uut67APwegHeo6kRy3t0A3qmq/yw5/RCAb6nqn2z82PPnz+uxY8euO8dO3H/u0qtOvya/iJ8lz8x+8PhRt6+TJePj4xgcHAw9RtTY0Ac72rGhD0vHCxcuPD0yMnLLVpc1clf2kwBuEpEbRaQA4C4AZzZeQUTeAuAhALe/signzgF4j4j0Jk/6ek9y3p5qF74puNXSEh8OsGJDH+xox4Y+0uq47V3ZqroiIvdibUHNAXhYVZ8RkQcAPKWqZwD8DoBOAH8mIgDwoqrerqrTIvIg1hZ3AHhAVadTOZLr4D5mO+57tGNDH+xox4Y+gu5jVtWzqvpLqnpUVT+TnPfJZFGGqr5LVQdV9c3Jn9s3fOzDqvrG5M8fpnIU2+A+Zjvue7RjQx/saMeGPvh+zAZVzYUeIXptbW2hR4geG/pgRzs29JFWx0wszIurrk8+z6T29vbQI0SPDX2wox0b+kirYyYW5v7c1dAjRG9mZib0CNFjQx/saMeGPtLqmImFuVzn3TZW/f39oUeIHhv6YEc7NvSRVsdMLMzdLXzvUau5ubnQI0SPDX2wox0b+kirYyYW5gOyGnqE6PGN1e3Y0Ac72rGhj7Q6ZmJh5j5mO+57tGNDH+xox4Y+gu5jjh33Mdtx36MdG/pgRzs29MF9zAbcLmXH7RV2bOiDHe3Y0Ae3SxksaSYOM1V8Y3U7NvTBjnZs6COtjpm4KdmXq2Fm9UDoMaJWqVRwww03hB4jas3acPO7uW2Uxru5NWvHvcSGPtLqmImbkuMr3MdsNTAwEHqE6LGhD3a0Y0MfaXXMxMLcl+PWAKtKpRJ6hOixoQ92tGNDH2l1zMTC3Mp9zGbLy8uhR4geG/pgRzs29JFWx0wszNzHbMd9j3Zs6IMd7djQB/cxG3Afsx33PdqxoQ92tGNDH9zHbDDPfcxmHR0doUeIHhv6YEc7NvSRVsdMLMwr2TjMVOVyudAjRI8NfbCjHRv6SKtjJlasG/juUmazs7OhR4geG/pgRzs29JFWx0wszJdX+PJzVsViMfQI0WNDH+xox4Y+0uqYiYW5mFsKPUL0pqenQ48QPTb0wY52bOgjrY6ZWJhbJPQE8VPV0CNEjw19sKMdG/pIq2MmFubLfElOM971ZceGPtjRjg198K5sg8P5augRojc+Ph56hOixoQ92tGNDH2l1zMTCPLvaGnqE6HV2doYeIXps6IMd7djQR1odM7EwExERxSITC3N3C1+w3Wp+fj70CNFjQx/saMeGPtLqmImF+WXuYzYbHBwMPUL02NAHO9qxoY+0OmZiYT6U5z5mq3K5HHqE6LGhD3a0Y0MfaXVsaGEWkRMi8pyIXBSR+7a4/O0ickFEVkTkzk2X1UXk+8mfM16D78Qqt+yZiXAzuBUb+mBHOzb0kVbHbd92SURyAL4A4N0AxgA8KSJnVPXZDVd7EcCHAXx8i09RVdU3O8y6a+U69zFb9fX1hR4hemzogx3t2NBHWh0bucV8K4CLqvq8qtYAPALgjo1XUNUXVPWHAFZTmNHsEPcxm/GuLzs29MGOdmzoI+Rd2YcBvLTh9FhyXqPaROQpEfmOiLx/R9M5ubJaCPFlm0p3d3foEaLHhj7Y0Y4NfaTVcdu7sh0Mq+rLIvIGAN8UkR+p6qWNV5iYmMCpU6eQz+dRr9dx8uRJnD59GqVSCR0dHcjlcpidnUWxWMT09DRUFcViEePj4+sbvOfn5zE4OIhyuYzh/DzK9TYcyldxZbWA/pYl3NBSw9jKQYyOjqK1tRU9PT2YnJxET08ParUaqtUqhoaGUCqVUCgU0NXVhampKfT29qJarWJpaWn98ra2NrS3t2NmZgb9/f2Ym5tDrVZbv7y9vR2FQgGVSgUDAwOoVCpYXl5ev3w3xyQi6OvrQ7lcRnd3N+r1OhYWFtY/Z9rHtLKyAhFpqmMK8X2qVqtNd0yvyy9gul7AYH4J0/UC2mQVB1tW1n/evI+pXC5jdnaW//YMxwQAy8vLTXVMIb5P9XodAHZ1TNcj270It4i8FcCnVfV4cvoTAKCqn93iul8C8HVVfewan2vLy8+fP6/Hjh277hw7cf+5V637ONo6h0vLXQCAB48fdfs6WTI6Oorh4eHQY0StWRtu/nnbKI2ft2btuJfY0Iel44ULF54eGRm5ZavLGrkr+0kAN4nIjSJSAHAXgIaeXS0ivSJyIPn7AIC3AXj2+h/lb2zl4F5/yaYzNDQUeoTosaEPdrRjQx9pddx2YVbVFQD3AjgH4CcAHlXVZ0TkARG5HQBE5NdFZAzABwA8JCLPJB/+ywCeEpEfAHgCwOc2PZt7TxzJL+71l2w6pVIp9AjRY0Mf7GjHhj7S6tjQY8yqehbA2U3nfXLD358EcGSLj/s2gF81zmi2rJl4HZVUtbbyjUCs2NAHO9qxoY+0OmZixZqu81nZVj09PaFHiB4b+mBHOzb0kVbHTCzMg3xJTrNXnslJu8eGPtjRjg19pNUxEwszbzHb8TdsOzb0wY52bOiDt5gN2mRfviBZVGq1WugRoseGPtjRjg19pNUxEwvzwZaV0CNEr1rly5pasaEPdrRjQx9pdczEwsx9zHbc92jHhj7Y0Y4NfQTbx9wMuI/Zjvse7djQBzvasaGPtDpmYmG+yn3MZoUCn0BnxYY+2NGODX2k1TETK9Ys313KrKurK/QI0WNDH+xox4Y+0uqYiYW5mOM+ZqupqanQI0SPDX2wox0b+kirYyYW5qn6gdAjRK+3tzf0CNFjQx/saMeGPtLqmImFmdul7Li9wo4NfbCjHRv64HYpg3aphx4hektLfDjAig19sKMdG/pIq2MmFmbuY7bjvkc7NvTBjnZs6COtjg297WPsjuQXcWn558+eu//cpWte98HjR/dipOiUSiUMDw+HHiNqbOiDHe3Y0EdaHTNxi7mqudAjRK+trS30CNFjQx/saMeGPtLqmImFeXE1E3cMpKq9vT30CNFjQx/saMeGPtLqmImFuT93NfQI0ZuZmQk9QvTY0Ac72rGhj7Q6ZmJhLtd5t41Vf39/6BGix4Y+2NGODX2k1TETC3N3C9971Gpubi70CNFjQx/saMeGPtLqmImF+YCshh4henxjdTs29MGOdmzoI62OmViYuY/Zjvse7djQBzvasaEPvh+zAd+P2Y7v32rHhj7Y0Y4NffD9mA24XcqO2yvs2NAHO9qxoQ9ulzJY0kwcZqr4xup2bOiDHe3Y0EdaHTOxYvXl+EQHq0qlEnqE6LGhD3a0Y0MfaXXMxMI8vsJ9zFYDAwOhR4geG/pgRzs29JFWx0wszLzFbMffsO3Y0Ac72rGhD95iNmjlPmaz5eXl0CNEjw19sKMdG/pIq2NDC7OInBCR50Tkoojct8XlbxeRCyKyIiJ3brrsHhH5afLnHq/Bd4L7mO2479GODX2wox0b+gi2j1lEcgC+AOC9AG4GcLeI3Lzpai8C+DCAP970sX0APgXgNgC3AviUiPTax94Z7mO2475HOzb0wY52bOgj5D7mWwFcVNXnVbUG4BEAd2y8gqq+oKo/BLD5PuPjAB5X1WlVnQHwOIATDnPvyDz3MZt1dHSEHiF6bOiDHe3Y0EdaHRtZsQ4DeGnD6TGs3QJuxFYfe3jzlSYmJnDq1Cnk83nU63WcPHkSp0+fRqlUQkdHB3K5HGZnZ1EsFjE9PQ1VRbFYxPj4ODo7OwEA8/PzGBwcRLlcxnB+HuV6Gw7lq7iyWkBnywo6W+YwtnIQo6OjeF1+AdP1AgbzS5iuF9AmqzjYsrJ+eaFQQFdXF6amptDb24tqtYqlpSUMDQ2hVCqhra0N7e3tmJmZQX9/P+bm5lCr1dYvb29vR6FQQKVSwcDAACqVCpaXl9cv380xiQj6+vpQLpfR3d2Ner2OhYWF9c/Z2tqKnp4eTE5OoqenB7VaDdVqdf1y6zGtrq4in8831THt9fdpamoKtVqtqY6pVCrt+c/TlStXXvX1+W9v58eUy+UwMTHRVMcU4vsEYNfHdD2iqte/wtpjxidU9SPJ6Q8BuE1V793iul8C8HVVfSw5/XEAbar675PT9wOoqup/2Phx58+f12PHjl13jp24/9ylV50+2jqHS8tdAIAHjx/9hcs3evD4Ubc5msno6CiGh4dDjxG1Zm241z9PzdpxL7GhD0vHCxcuPD0yMnLLVpc1clf2ywBeu+H0keS8Rlg+1s3lFb78nFWxWAw9QvTY0Ac72rGhj7Q6NrIwPwngJhG5UUQKAO4CcKbBz38OwHtEpDd50td7kvP2VDG3tNdfsulMT0+HHiF6bOiDHe3Y0EdaHbddmFV1BcC9WFtQfwLgUVV9RkQeEJHbAUBEfl1ExgB8AMBDIvJM8rHTAB7E2uL+JIAHkvP2VIvs9VdsPts95EHbY0Mf7GjHhj7S6tjQ05VV9SyAs5vO++SGvz+Jtbupt/rYhwE8bJjR7DJfktOMd33ZsaEPdrRjQx8h78qO3uF8NfQI0RsfHw89QvTY0Ac72rGhj7Q6ZmJhnl1tDT1C9F552j/tHhv6YEc7NvSRVsdMLMxERESxyMTC3N3CF2y3mp+fDz1C9NjQBzvasaGPtDpmYmF+mfuYzQYHB0OPED029MGOdmzoI62OmViYD+W5j9mqXC6HHiF6bOiDHe3Y0EdaHTOxMK9yy56ZCDeDW7GhD3a0Y0MfaXXMxMJcrnMfs9V2L7pO22NDH+xox4Y+0uqYiYX5EPcxm/GuLzs29MGOdmzog3dlG1xZLYQeIXrd3d2hR4geG/pgRzs29JFWx0wszHmshh4hevV6PfQI0WNDH+xox4Y+0uqYiYW5s2Ul9AjRW1hYCD1C9NjQBzvasaGPtDpmYmEeWzkYeoToDQ0NhR4hemzogx3t2NBHWh0zsTAfyS+GHiF6pVIp9AjRY0Mf7GjHhj7S6piJhXlZM3GYqWpt5RuBWLGhD3a0Y0MfaXXMxIo1Xeezsq16enpCjxA9NvTBjnZs6COtjplYmAf5kpxmk5OToUeIHhv6YEc7NvSRVsdMLMy8xWzH37Dt2NAHO9qxoQ/eYjZoE+5jtqrVaqFHiB4b+mBHOzb0kVbHTCzMB7mP2axa5cuaWrGhD3a0Y0MfaXXMxMLMfcx23Pdox4Y+2NGODX1wH7MB9zHbcd+jHRv6YEc7NvTBfcwGV7mP2axQ4BPorNjQBzvasaGPtDpmYsWa5btLmXV1dYUeIXps6IMd7djQR1odM7EwF3Pcx2w1NTUVeoTosaEPdrRjQx9pdczEwjxVPxB6hOj19vaGHiF6bOiDHe3Y0EdaHTOxMHO7lB23V9ixoQ92tGNDH9wuZdAufFNwq6UlPhxgxYY+2NGODX2k1TETCzP3Mdtx36MdG/pgRzs29BF0H7OInBCR50Tkoojct8XlB0TkT5PLvysir0/Of72IVEXk+8mf3/cdvzHcx2zHfY92bOiDHe3Y0EdaHfPbXUFEcgC+AODdAMYAPCkiZ1T12Q1XOwVgRlXfKCJ3AfhtAL+VXHZJVd/sPPeOVDUX8ss3hba2ttAjRI8NfbCjHRv6SKtjI7eYbwVwUVWfV9UagEcA3LHpOncA+HLy98cAjIiI+I1ps7i67e8ftI329vbQI0SPDX2wox0b+kirYyMr1mEAL204PQbgtmtdR1VXRKQCoD+57EYR+R6AWQD/VlX/evMXmJiYwKlTp5DP51Gv13Hy5EmcPn0apVIJHR0dyOVymJ2dRbFYxPT0NFQVxWIR4+Pj6OzsBADMz89jcHAQ5XIZw/l5lOttOJSv4spqAYdzi+jXqxhbOYjR0VG8Lr+A6XoBg/klTNcLaJNVHGxZWb+8UCigq6sLU1NT6O3tRbVaxdLSEoaGhlAqldDW1ob29nbMzMygv78fc3NzqNVq65e3t7ejUCigUqlgYGAAlUoFy8vL65fv5phEBH19fSiXy+ju7ka9XsfCwsL652xtbUVPTw8mJyfR09ODWq2GarW6frn1mBYWFvCa17ymqY5pr79PL7zwAgYGBprqmEql0p7/PL344ovo6urivz3DMS0tLWFpaampjinE92lxcRH1en1Xx3Q9oqrXv4LInQBOqOpHktMfAnCbqt674To/Tq4zlpy+hLXFew5Ap6pOicivAfgagF9R1dmNX+P8+fN67Nix686xE/efu/Sq090ty5hdbQUAPHj86C9cvtGDx4+6zdFM5ufn1/+R0e40a8O9/nlq1o57iQ19WDpeuHDh6ZGRkVu2uqyRu7JfBvDaDaePJOdteR0RyQPoATClqldVdQoAVPVpAJcA/NLOxrfrbuF7j1rNzc2FHiF6bOiDHe3Y0EdaHRtZmJ8EcJOI3CgiBQB3ATiz6TpnANyT/P1OAN9UVRWRYvLkMYjIGwDcBOB5n9Ebd0BW9/pLNh2+sbodG/pgRzs29JFWx20fY04eM74XwDkAOQAPq+ozIvIAgKdU9QyALwL4byJyEcA01hZvAHg7gAdEZBnAKoB/rqrTaRzI9XAfsx33PdqxoQ92tGNDH2l1bOjpyqp6FsDZTed9csPflwB8YIuP+wqArxhnNDuSX8SlZd93AbnW42rN+hh1qVTC8PBw6DGixoY+2NGODX2k1TETr/zF7VJ23F5hx4Y+2NGODX2k1TETC/OSZuIwU8U3VrdjQx/saMeGPtLqmIkVqy/HJzpYVSqV0CNEjw19sKMdG/pIq2MmFubxFb78nNXAwEDoEaLHhj7Y0Y4NfaTVMRMLM28x2/E3bDs29MGOdmzog7eYDVq5j9lseXk59AjRY0Mf7GjHhj7S6piJhZn7mO2479GODX2wox0b+gj6fsyx4/sx2/H9W+3Y0Ac72rGhj2Dvx9wM5ptwH/Nev3FAR0eH++fMGjb0wY52bOgjrY7Nt2JtYWUXdwxYX9mr2V4ZLJfLhR4hemzogx3t2NBHWh0zcVf2DXx3KbPZ2dntr0TXxYY+2NGODX2k1TETC/PlFb78nFWxWAw9QvTY0Ac72rGhj7Q6ZmJhLuaWQo8QvenpPX9TsKbDhj7Y0Y4NfaTVMRMLc4uEniB+qhp6hOixoQ92tGNDH2l1zMTCfJkvyWnGu77s2NAHO9qxoQ/elW1wOF8NPUL0xsfHQ48QPTb0wY52bOgjrY6ZWJhnV1tDjxC9zs7O0CNEjw19sKMdG/pIq2MmFmYiIqJYZOIFRrpblujEgq8AAAb9SURBVFGux/U4816/std25ufn0d/fv+dft5mwoQ92tGNDH2l1zMTC/DL3MZsNDg6GHiF6bOiDHe3Y8Np28qqNaXXMxF3Zh/Lcx2xVLpdDjxA9NvTBjnZs6COtjplYmFe5Zc9MhJvBrdjQBzvasaGPtDpm4q7s/fj48n54DHknd9n09fWlPU7TY0Mf7GjHhj7S6piJhflQvopLy12hx4hauVzG8PCw6+fc7peT/fDLi6c0GmYRO9qxoY+0OmZiYb6yWgg9QvS6u7t/4bxme2vLtG3VkHaOHe3Y0EdaHTOxMOexGnqE6NXr9dAjRG+/NoztF6z92jEmbOgjrY6ZWJg7W1Ywzn+HJgsLCxgYGAg9RtTSahjbwmrVrP8W9/L72KwN91paHTOxMI+tHAw9QvSGhoZCjxC9rRo22+Poe4H/Fu3Y0EdaHTOxMB/JL/LJX0alUolPFjGKteF+++VhP3bcD412MsNWDffDMcQmrX+LDS3MInICwO8CyAH4A1X93KbLDwD4IwC/BmAKwG+p6gvJZZ8AcApAHcC/UNVzbtM36G/+8i9w6B/ctddfNiiPH7KNn+PyE4+sN9zNx+92Buvn3+7uwbQuf+U612q48XNsx3oXZ+i7ur2/T1v9W/T8Pu3m8kaE/j5a/y3G9vO2m8t36mtf+xo++tGP7vjjtrPtC4yISA7AFwC8F8DNAO4WkZs3Xe0UgBlVfSOA/wTgt5OPvRnAXQB+BcAJAP8l+Xx76tv/+y/2+ks2HTa0Y0Mf7GjHhj6++tWvpvJ5G3nlr1sBXFTV51W1BuARAHdsus4dAL6c/P0xACOy9pIodwB4RFWvqur/A3Ax+Xx7is/KtmNDOzb0wY52bOhjZWUllc8rqtd/vUoRuRPACVX9SHL6QwBuU9V7N1znx8l1xpLTlwDcBuDTAL6jqv89Of+LAL6hqo9t/Bpnz56du3z58vovCd3d3eW+vr5Jh+MDAExPTw94fr4sYkM7NvTBjnZs6MPYcXhkZKS41QX74slf73vf+/jMLCIiIjR2V/bLAF674fSR5LwtryMieQA9WHsSWCMfS0RERIlGFuYnAdwkIjeKSAFrT+Y6s+k6ZwDck/z9TgDf1LX7yM8AuEtEDojIjQBuAvC3PqMTERE1n20XZlVdAXAvgHMAfgLgUVV9RkQeEJHbk6t9EUC/iFwE8DEA9yUf+wyARwE8C+B/ATitqnv2GlwickJEnhORiyJy31593diJyMMiMpE8d+CV8/pE5HER+Wny396QM+53IvJaEXlCRJ4VkWdE5KPJ+ezYIBFpE5G/FZEfJA3/XXL+jSLy3eTn+k+TGwy0DRHJicj3ROTryWl23AEReUFEfiQi3xeRp5LzUvl53vbJX7FKtmX9XwDvBjCGtVv+d6vqs0EHi4CIvB3APIA/UtW/n5z3eQDTqvq55JecXlX91yHn3M9E5BCAQ6p6QUS6ADwN4P0APgx2bEiys6NDVedFpBXA3wD4KNZ++f+qqj4iIr8P4Aeq+l9DzhoDEfkYgFsAdKvqb4rIo2DHhonICwBuUdXJDeel8v/FRu7KjlUj27xoC6r6fwBMbzp745a4L2NtkaFrUNXLqnoh+fsc1u5tOgx2bJiumU9OtiZ/FMA/xNq2TIANGyIiRwD8IwB/kJwWsKOHVH6em3lhPgzgpQ2nx5LzaHcGVfVy8vcSgMGQw8RERF4P4C0Avgt23JHk7tfvA5gA8DiASwCuJA+xAfy5btR/BvCvgPUNzP1gx51SAH8pIk+LyD9Nzkvl53lfbJeiuKiqikhzPgbiTEQ6AXwFwL9U1dm1Gypr2HF7yXNS3iwiNwD4cwDHAo8UHRH5TQATqvq0iLwz9DwR+w1VfVlE/h6Ax0Xk7zZe6Pnz3My3mLlVy9d48rjpK4+fTgSeZ99LHhf9CoD/oaqvvHYfO+6Cql4B8ASAtwK4IdmWCfDnuhFvA3B78hjpI1i7C/t3wY47oqovJ/+dwNovibcipZ/nZl6YG9nmRY3buCXuHgD/M+As+17yGN4XAfxEVf/jhovYsUEiUkxuKUNE2rH2RM6fYG2BvjO5GhtuQ1U/oapHVPX1WPv/4DdV9Z+AHRsmIh3JkzghIh0A3gPgx0jp57lpn5UNACLyPqw9tpID8LCqfibwSFEQkT8B8E4AAwDGAXwKwNewtvXtdQBGAfxjVd38BDFKiMhvAPhrAD/Czx/X+zdYe5yZHRsgIm/C2hNqcli7EfGoqj4gIm/A2i2/PgDfA/BBVb0abtJ4JHdlfzx5VjY7Nihp9efJyTyAP1bVz4hIP1L4eW7qhZmIiCg2zXxXNhERUXS4MBMREe0jXJiJiIj2ES7MRERE+wgXZiIion2ECzMREdE+woWZiIhoH+HCTEREtI/8f6O6FxXon+x9AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# loss trajectory를 확인한다.\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label='Test loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 최적 포트폴리오 결과 조회용 코드\n",
        "def check_w(n = 0):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    y_pred = model.predict(xc_test[n].reshape(1, N_TIME, N_STOCKS))[0]\n",
        "    plt.bar(np.arange(N_STOCKS), y_pred, alpha = 0.7)\n",
        "    plt.show()\n",
        "\n",
        "check_w(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_P4ka7nBBfo",
        "outputId": "5c0db01d-9e82-4b6e-cb71-45015f76fe6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 60, 50)]          0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                29440     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 50)                3250      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 50)                0         \n",
            "=================================================================\n",
            "Total params: 32,690\n",
            "Trainable params: 32,690\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "N_TIME = xc_test.shape[1]\n",
        "N_FUTURE = xf_test.shape[1]\n",
        "N_STOCKS = xf_test.shape[2]\n",
        "\n",
        "# 저장된 Markowitz 모델을 가져온다.\n",
        "SAVE_MODEL = 'Markowitz_network_final.h5'\n",
        "model = load_model(SAVE_MODEL, compile = True, custom_objects={'markowitz_objective': markowitz_objective})\n",
        "model.summary()\n",
        "\n",
        "# 백 테스트를 수행한다.\n",
        "prt_value = [10000]   # portfolio의 초기 value\n",
        "crp_value = [10000]   # CRP의 초기 value\n",
        "mvstd_value = [10000]    # MV_std의 초기 value\n",
        "mvshp_value = [10000] # MV_sharpe의 초기 value\n",
        "mvVaR_value = [10000] # MV_VaR의 초기 value\n",
        "w_crp = np.ones(N_STOCKS) / N_STOCKS   # CRP 비율 (균등 비율)\n",
        "\n",
        "w_history = []\n",
        "w_history_std = []\n",
        "w_history_shp = []\n",
        "w_history_VaR = []\n",
        "for i in range(0, xc_test.shape[0], N_FUTURE):\n",
        "   \n",
        "    # 이 시점에 각 종목을 w_prt 비율대로 매수한다.\n",
        "    # 학습할 때 월간 수익률로 변환했으므로, 여기서도 변환해야 한다.\n",
        "    x = xc_test[i][np.newaxis,:,:] * 20\n",
        "    w_prt = model.predict(x)[0]\n",
        "    w_history.append(w_prt)\n",
        "    \n",
        "    # 추가 코드\n",
        "    x_mean = np.mean(x[0], axis=0)\n",
        "    x_cov = np.cov(x[0].T)\n",
        "    \n",
        "    result_std = portfolio_optimization_std(w_crp, x_mean, x_cov)\n",
        "    w_std = np.array([round(x,3) for x in result_std['x']])\n",
        "    \n",
        "    result_sharpe = portfolio_optimization_sharpe(w_crp, x_mean, x_cov, 0)\n",
        "    w_shp = np.array([round(x,3) for x in result_sharpe['x']])\n",
        "    \n",
        "    result_VaR = portfolio_optimization_sharpe(w_crp, x_mean, x_cov, 0.05)\n",
        "    w_VaR = np.array([round(x,3) for x in result_VaR['x']])\n",
        "    \n",
        "    w_history_std.append(w_std)\n",
        "    w_history_shp.append(w_shp)\n",
        "    w_history_VaR.append(w_VaR)\n",
        "\n",
        "    # 다음 기간의 누적 수익률\n",
        "    m_rtn = np.sum(xf_test[i], axis = 0) / 20\n",
        " \n",
        "    # 누적 수익률과 w_prt (W)로 포트폴리오의 수익률을 계산한다.\n",
        "    prt_value.append(prt_value[-1] * np.exp(np.dot(w_prt, m_rtn)))\n",
        "    crp_value.append(crp_value[-1] * np.exp(np.dot(w_crp, m_rtn)))\n",
        "    \n",
        "    mvstd_value.append(mvstd_value[-1] * np.exp(np.dot(w_std, m_rtn)))\n",
        "    mvshp_value.append(mvshp_value[-1] * np.exp(np.dot(w_shp, m_rtn)))\n",
        "    mvVaR_value.append(mvVaR_value[-1] * np.exp(np.dot(w_VaR, m_rtn)))\n",
        "\n",
        "    # 추가로 발생한 시장 데이터로 MPN을 추가 학습시킨다.\n",
        "    # xc_test[0] ~ xc_test[19],\n",
        "    # xf_test[0] ~ xf_test[19]를 추가로 학습시킬 수 있다.\n",
        "    xc_new = xc_test[i:(i+N_FUTURE), :, :]\n",
        "    xf_new = xf_test[i:(i+N_FUTURE), :, :]\n",
        "\n",
        "    # xc_train 데이터에서 80개를 random sampling 한다.\n",
        "    idx = np.random.randint(0, xc_train.shape[0], 80)\n",
        "\n",
        "    # 추가 학습 데이터를 생성한다.\n",
        "    x = np.vstack([xc_new, xc_train[idx]])\n",
        "    y = np.vstack([xf_new, xf_train[idx]])\n",
        "    x, y = shuffle(x, y)\n",
        "\n",
        "    # 추가 학습한다.\n",
        "    x *= 20.0\n",
        "    y *= 20.0\n",
        "    model.fit(x, y, epochs=50, batch_size=10, verbose=0)\n",
        "\n",
        "    # 추가로 발생한 데이터도 이후에 sampling될 수 있도록 보관해 둔다.\n",
        "    xc_train = np.vstack([xc_train, xc_new])\n",
        "    xf_train = np.vstack([xf_train, xf_new])\n",
        "\n",
        "# 평가 시점의 날짜를 발췌한다.\n",
        "idx = np.arange(0, xc_test.shape[0] + 20, N_FUTURE)\n",
        "\n",
        "# 벤치마크를 위한 S&P500 정의\n",
        "us500 = fdr.DataReader('US500', '2002-01-01', '2022-12-01')\\\n",
        "    ['Adj Close'][-993:][idx]\n",
        "us500_value = us500 / us500[0] * 10000\n",
        "\n",
        "# Markowitz 성과와 CRP 성과를 데이터 프레임에 기록해 둔다.\n",
        "perf_df = pd.DataFrame({'crp':crp_value, 'markowitz':prt_value, 'MV_Std':mvstd_value, 'MV_Max_Shp':mvshp_value, 'MV_VaR':mvVaR_value,'S&P500':us500_value})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "K6GNFJGJDYKb"
      },
      "outputs": [],
      "source": [
        "model.save(SAVE_MODEL + 'AdLearning.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6RVcdViBBfp"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2005년부터 2022년까지 학습, 2002년~2005년 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1053, 4132)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(colab_path + \"top50-Stock_LogReturn.csv\", index_col=\"Date\")\n",
        "train, test = df[1053:], df[:1053]\n",
        "\n",
        "train_len = len(train)\n",
        "test_len = len(test)\n",
        "validation_set_len = 80\n",
        "validation_set_split_point = 60\n",
        "\n",
        "xc_train = np.empty((train_len - validation_set_len, 60, 50))\n",
        "xf_train = np.empty((train_len - validation_set_len, 20, 50))\n",
        "xc_test = np.empty((test_len - validation_set_len, 60, 50))\n",
        "xf_test = np.empty((test_len - validation_set_len, 20, 50))\n",
        "\n",
        "for idx in range(train_len - validation_set_len):\n",
        "    temp_xc_train = train[idx : idx + validation_set_split_point]\n",
        "    temp_xf_train = train[idx + validation_set_split_point : idx + validation_set_len]\n",
        "\n",
        "    xc_train[idx] = temp_xc_train\n",
        "    xf_train[idx] = temp_xf_train\n",
        "\n",
        "for idx in range(test_len - validation_set_len):\n",
        "    temp_xc_test = test[idx : idx + validation_set_split_point]\n",
        "    temp_xf_test = test[idx + validation_set_split_point : idx + validation_set_len]\n",
        "\n",
        "    xc_test[idx] = temp_xc_test\n",
        "    xf_test[idx] = temp_xf_test\n",
        "    \n",
        "# 월간 수익률 정도의 스케일로 변환한다\n",
        "xc_train = xc_train.astype('float32') * 20\n",
        "xf_train = xf_train.astype('float32') * 20\n",
        "xc_test = xc_test.astype('float32') * 20\n",
        "xf_test = xf_test.astype('float32') * 20\n",
        "\n",
        "N_TIME = xc_train.shape[1]\n",
        "N_FUTURE = xf_train.shape[1]\n",
        "N_STOCKS = xf_train.shape[2]\n",
        "\n",
        "# 학습 데이터는 shuffling 한다.\n",
        "xc_train, xf_train = shuffle(xc_train, xf_train)\n",
        "test_len, train_len - validation_set_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM으로 Markowitz 모델을 생성한다.\n",
        "xc_input = tf.keras.Input(batch_shape = (None, N_TIME, N_STOCKS))\n",
        "h_lstm = LSTM(64, dropout = 0.2, kernel_regularizer=l2(0.01))(xc_input)\n",
        "y_output = Dense(N_STOCKS, activation='tanh')(h_lstm)  # linear projection\n",
        "\n",
        "# 특정 종목을 과도하게 매수하는 것을 방지하기 위해 위에서 tanh를 사용했다. \n",
        "# (over confidence 방지용). REG_CONST를 적용했기 때문에 이미 고려된 사항이지만, \n",
        "# 안전을 위해 추가했다. ex : [-3, 0.4, 0.2, +20] --> [-0.995, 0.380, 0.197, 1.0]|\n",
        "\n",
        "# 마코비츠의 최적 weights\n",
        "y_output = Activation('softmax')(y_output)\n",
        "model = tf.keras.Model(xc_input, y_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras import callbacks\n",
        "# # MPN을 학습하고 결과를 저장한다.\n",
        "SAVE_MODEL = 'Markowitz_network_CV_final'\n",
        "ealry_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "model.compile(loss = markowitz_objective, optimizer = Adam(learning_rate = 1e-6))\n",
        "hist = model.fit(xc_train, xf_train, epochs=500, batch_size = 32, validation_data = (xc_test, xf_test), callbacks=[ealry_stopping])\n",
        "model.save(SAVE_MODEL + '.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# loss trajectory를 확인한다.\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label='Test loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 최적 포트폴리오 결과 조회용 코드\n",
        "def check_w(n = 0):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    y_pred = model.predict(xc_test[n].reshape(1, N_TIME, N_STOCKS))[0]\n",
        "    plt.bar(np.arange(N_STOCKS), y_pred, alpha = 0.7)\n",
        "    plt.show()\n",
        "\n",
        "check_w(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_TIME = xc_test.shape[1]\n",
        "N_FUTURE = xf_test.shape[1]\n",
        "N_STOCKS = xf_test.shape[2]\n",
        "\n",
        "# 저장된 Markowitz 모델을 가져온다.\n",
        "SAVE_MODEL = 'Markowitz_network_CV_final.h5'\n",
        "model = load_model(SAVE_MODEL, compile = True, custom_objects={'markowitz_objective': markowitz_objective})\n",
        "model.summary()\n",
        "\n",
        "# 백 테스트를 수행한다.\n",
        "prt_value = [10000]   # portfolio의 초기 value\n",
        "crp_value = [10000]   # CRP의 초기 value\n",
        "mvstd_value = [10000]    # MV_std의 초기 value\n",
        "mvshp_value = [10000] # MV_sharpe의 초기 value\n",
        "mvVaR_value = [10000] # MV_VaR의 초기 value\n",
        "w_crp = np.ones(N_STOCKS) / N_STOCKS   # CRP 비율 (균등 비율)\n",
        "\n",
        "w_history = []\n",
        "w_history_std = []\n",
        "w_history_shp = []\n",
        "w_history_VaR = []\n",
        "for i in range(0, xc_test.shape[0], N_FUTURE):\n",
        "   \n",
        "    # 이 시점에 각 종목을 w_prt 비율대로 매수한다.\n",
        "    # 학습할 때 월간 수익률로 변환했으므로, 여기서도 변환해야 한다.\n",
        "    x = xc_test[i][np.newaxis,:,:] * 20\n",
        "    w_prt = model.predict(x)[0]\n",
        "    w_history.append(w_prt)\n",
        "    \n",
        "    # 추가 코드\n",
        "    x_mean = np.mean(x[0], axis=0)\n",
        "    x_cov = np.cov(x[0].T)\n",
        "    \n",
        "    result_std = portfolio_optimization_std(w_crp, x_mean, x_cov)\n",
        "    w_std = np.array([round(x,3) for x in result_std['x']])\n",
        "    \n",
        "    result_sharpe = portfolio_optimization_sharpe(w_crp, x_mean, x_cov, 0)\n",
        "    w_shp = np.array([round(x,3) for x in result_sharpe['x']])\n",
        "    \n",
        "    result_VaR = portfolio_optimization_sharpe(w_crp, x_mean, x_cov, 0.05)\n",
        "    w_VaR = np.array([round(x,3) for x in result_VaR['x']])\n",
        "    \n",
        "    w_history_std.append(w_std)\n",
        "    w_history_shp.append(w_shp)\n",
        "    w_history_VaR.append(w_VaR)\n",
        "\n",
        "    # 다음 기간의 누적 수익률\n",
        "    m_rtn = np.sum(xf_test[i], axis = 0) / 20\n",
        " \n",
        "    # 누적 수익률과 w_prt (W)로 포트폴리오의 수익률을 계산한다.\n",
        "    prt_value.append(prt_value[-1] * np.exp(np.dot(w_prt, m_rtn)))\n",
        "    crp_value.append(crp_value[-1] * np.exp(np.dot(w_crp, m_rtn)))\n",
        "    \n",
        "    mvstd_value.append(mvstd_value[-1] * np.exp(np.dot(w_std, m_rtn)))\n",
        "    mvshp_value.append(mvshp_value[-1] * np.exp(np.dot(w_shp, m_rtn)))\n",
        "    mvVaR_value.append(mvVaR_value[-1] * np.exp(np.dot(w_VaR, m_rtn)))\n",
        "\n",
        "    # 추가로 발생한 시장 데이터로 MPN을 추가 학습시킨다.\n",
        "    # xc_test[0] ~ xc_test[19],\n",
        "    # xf_test[0] ~ xf_test[19]를 추가로 학습시킬 수 있다.\n",
        "    xc_new = xc_test[i:(i+N_FUTURE), :, :]\n",
        "    xf_new = xf_test[i:(i+N_FUTURE), :, :]\n",
        "\n",
        "    # xc_train 데이터에서 80개를 random sampling 한다.\n",
        "    idx = np.random.randint(0, xc_train.shape[0], 80)\n",
        "\n",
        "    # 추가 학습 데이터를 생성한다.\n",
        "    x = np.vstack([xc_new, xc_train[idx]])\n",
        "    y = np.vstack([xf_new, xf_train[idx]])\n",
        "    x, y = shuffle(x, y)\n",
        "\n",
        "    # 추가 학습한다.\n",
        "    x *= 20.0\n",
        "    y *= 20.0\n",
        "    model.fit(x, y, epochs=50, batch_size=10, verbose=0)\n",
        "\n",
        "    # 추가로 발생한 데이터도 이후에 sampling될 수 있도록 보관해 둔다.\n",
        "    xc_train = np.vstack([xc_train, xc_new])\n",
        "    xf_train = np.vstack([xf_train, xf_new])\n",
        "\n",
        "# 평가 시점의 날짜를 발췌한다.\n",
        "idx = np.arange(0, xc_test.shape[0] + 20, N_FUTURE)\n",
        "\n",
        "# 벤치마크를 위한 S&P500 정의\n",
        "us500 = fdr.DataReader('US500', '2002-01-01', '2022-12-01')\\\n",
        "    ['Adj Close'][1:993][idx]\n",
        "us500_value = us500 / us500[0] * 10000\n",
        "\n",
        "# Markowitz 성과와 CRP 성과를 데이터 프레임에 기록해 둔다.\n",
        "perf_df = pd.DataFrame({'crp':crp_value, 'markowitz':prt_value, 'MV_Std':mvstd_value, 'MV_Max_Shp':mvshp_value, 'MV_VaR':mvVaR_value,'S&P500':us500_value})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(SAVE_MODEL + 'AdLearning.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 예언자 Prophet 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1053, 5185)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(colab_path + \"top50-Stock_LogReturn.csv\", index_col=\"Date\")\n",
        "train, test = df, train_test_split(df, test_size=0.2, shuffle=False)[1]\n",
        "\n",
        "train_len = len(train)\n",
        "test_len = len(test)\n",
        "validation_set_len = 80\n",
        "validation_set_split_point = 60\n",
        "\n",
        "xc_train = np.empty((train_len - validation_set_len, 60, 50))\n",
        "xf_train = np.empty((train_len - validation_set_len, 20, 50))\n",
        "xc_test = np.empty((test_len - validation_set_len, 60, 50))\n",
        "xf_test = np.empty((test_len - validation_set_len, 20, 50))\n",
        "\n",
        "for idx in range(train_len - validation_set_len):\n",
        "    temp_xc_train = train[idx : idx + validation_set_split_point]\n",
        "    temp_xf_train = train[idx + validation_set_split_point : idx + validation_set_len]\n",
        "\n",
        "    xc_train[idx] = temp_xc_train\n",
        "    xf_train[idx] = temp_xf_train\n",
        "\n",
        "for idx in range(test_len - validation_set_len):\n",
        "    temp_xc_test = test[idx : idx + validation_set_split_point]\n",
        "    temp_xf_test = test[idx + validation_set_split_point : idx + validation_set_len]\n",
        "\n",
        "    xc_test[idx] = temp_xc_test\n",
        "    xf_test[idx] = temp_xf_test\n",
        "    \n",
        "# 월간 수익률 정도의 스케일로 변환한다\n",
        "xc_train = xc_train.astype('float32') * 20\n",
        "xf_train = xf_train.astype('float32') * 20\n",
        "xc_test = xc_test.astype('float32') * 20\n",
        "xf_test = xf_test.astype('float32') * 20\n",
        "\n",
        "N_TIME = xc_train.shape[1]\n",
        "N_FUTURE = xf_train.shape[1]\n",
        "N_STOCKS = xf_train.shape[2]\n",
        "\n",
        "# 학습 데이터는 shuffling 한다.\n",
        "xc_train, xf_train = shuffle(xc_train, xf_train)\n",
        "test_len, train_len - validation_set_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM으로 Markowitz 모델을 생성한다.\n",
        "xc_input = tf.keras.Input(batch_shape = (None, N_TIME, N_STOCKS))\n",
        "h_lstm = LSTM(64, dropout = 0.2, kernel_regularizer=l2(0.01))(xc_input)\n",
        "y_output = Dense(N_STOCKS, activation='tanh')(h_lstm)  # linear projection\n",
        "\n",
        "# 특정 종목을 과도하게 매수하는 것을 방지하기 위해 위에서 tanh를 사용했다. \n",
        "# (over confidence 방지용). REG_CONST를 적용했기 때문에 이미 고려된 사항이지만, \n",
        "# 안전을 위해 추가했다. ex : [-3, 0.4, 0.2, +20] --> [-0.995, 0.380, 0.197, 1.0]|\n",
        "\n",
        "# 마코비츠의 최적 weights\n",
        "y_output = Activation('softmax')(y_output)\n",
        "model = tf.keras.Model(xc_input, y_output)\n",
        "\n",
        "# # MPN을 학습하고 결과를 저장한다.\n",
        "SAVE_MODEL = 'prophet'\n",
        "ealry_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "model.compile(loss = markowitz_objective, optimizer = Adam(learning_rate = 1e-6))\n",
        "hist = model.fit(xc_train, xf_train, epochs=500, batch_size = 32, validation_data = (xc_test, xf_test), callbacks=[ealry_stopping])\n",
        "model.save(SAVE_MODEL + '.h5')\n",
        "\n",
        "# loss trajectory를 확인한다.\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label='Test loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 최적 포트폴리오 결과 조회용 코드\n",
        "def check_w(n = 0):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    y_pred = model.predict(xc_test[n].reshape(1, N_TIME, N_STOCKS))[0]\n",
        "    plt.bar(np.arange(N_STOCKS), y_pred, alpha = 0.7)\n",
        "    plt.show()\n",
        "\n",
        "check_w(0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "finance",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "47ec0ad232d3e10df23e169c0b34d647f49874296b3dcb8d8db7cb70009d8222"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
